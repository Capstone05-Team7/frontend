package com.example.capstone07.ui.speech

import android.Manifest
import android.content.pm.PackageManager
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.os.Bundle
import android.os.Handler
import android.os.Looper
import android.util.Log
import android.view.LayoutInflater
import android.view.View
import android.view.ViewGroup
import android.widget.Toast
import androidx.activity.result.ActivityResultLauncher
import androidx.activity.result.contract.ActivityResultContracts
import androidx.annotation.RequiresPermission
import androidx.core.content.ContextCompat
import androidx.fragment.app.Fragment
import com.example.capstone07.R
import com.example.capstone07.databinding.FragmentAnalysisBinding
import com.example.capstone07.remote.PresentationStompClient
import com.example.capstone07.remote.ProgressResponse
import com.example.capstone07.remote.SimilarityResponse
import com.google.api.gax.core.FixedCredentialsProvider
import com.google.api.gax.rpc.ApiStreamObserver
import com.google.auth.oauth2.GoogleCredentials
import com.google.protobuf.ByteString
import com.google.cloud.speech.v1.RecognitionConfig
import com.google.cloud.speech.v1.SpeechClient
import com.google.cloud.speech.v1.SpeechSettings
import com.google.cloud.speech.v1.StreamingRecognitionConfig
import com.google.cloud.speech.v1.StreamingRecognizeRequest
import com.google.cloud.speech.v1.StreamingRecognizeResponse

class AnalysisFragment : Fragment() {

    private var _binding: FragmentAnalysisBinding? = null
    private val binding get() = _binding!!

    // for Google Cloud STT
    private var speechClient: SpeechClient? = null
    private var requestObserver: ApiStreamObserver<StreamingRecognizeRequest>? = null

    // for AudioRecord
    private var audioRecord: AudioRecord? = null
    private val sampleRate = 16000 // STT APIê°€ ê¶Œì¥í•˜ëŠ” í‘œì¤€ ìƒ˜í”Œ ë ˆì´íŠ¸
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    private var bufferSize = 0

    // STTì— í•„ìš”í•œ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ìš©
    private lateinit var requestPermissionLauncher: ActivityResultLauncher<String>

    // ë§ˆì´í¬ ì¸ì‹ ìƒíƒœ
    private var isListening = false

    // ì›¹ì†Œì¼“ í´ë¼ì´ì–¸íŠ¸ ê°ì²´
    private lateinit var stompClient: PresentationStompClient
    // ë°œí‘œ ID (ì•„ë§ˆ projectIdë¥¼ ì“¸ ê²ƒ ê°™ì€ë°, íŠ¹ì • í”„ë¡œì íŠ¸ ì¡°íšŒ apiê°€ ì—†ì–´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í•˜ë“œì½”ë”©)
    private val PRESENTATION_ID = "1"

    private val TAG = "AnalysisFragment"

    // UI ìŠ¤ë ˆë“œì—ì„œ ë™ì‘í•  í•¸ë“¤ëŸ¬
    private val hintHandler = Handler(Looper.getMainLooper())

    // íƒ€ì´ë¨¸ê°€ ë§Œë£Œë˜ë©´ ì‹¤í–‰ë  Runnable
    private val hintTimerRunnable = Runnable {
        //Log.d(TAG, "2ì´ˆê°„ ì¹¨ë¬µ ê°ì§€. ì„œë²„ì— íŒíŠ¸ ìš”ì²­.")
        // TODO: PresentationStompClientì— "íŒíŠ¸ ìš”ì²­" ë©”ì„œë“œ êµ¬í˜„ í•„ìš”
        //       (stompClient.sendSttText() ì™€ëŠ” ë‹¤ë¥¸, íŒíŠ¸ë¥¼ ìš”ì²­í•˜ëŠ” ë³„ë„ ë©”ì‹œì§€ ì „ì†¡)
        if (::stompClient.isInitialized) {
            //stompClient.requestHint() // (ê°€ì •) íŒíŠ¸ ìš”ì²­ ë©”ì„œë“œ í˜¸ì¶œ
        }
    }
    // --- íŒíŠ¸ íƒ€ì´ë¨¸ ë¡œì§ ë ---

    private val recognizedSpeechBuffer = StringBuilder()
    // ğŸ’¡ ì¶”ê°€: ë²„í¼ ê´€ë¦¬ë¥¼ ìœ„í•œ ìƒìˆ˜
    private val MAX_WORD_COUNT = 20 // ìµœëŒ€ í—ˆìš© ë‹¨ì–´ ìˆ˜
    private val TRIM_WORD_COUNT = 10 // ì‚­ì œí•  ë‹¨ì–´ ìˆ˜ (MAX_WORD_COUNTì˜ ì ˆë°˜)

    private var speakingSentence: String = ""
    private var speakingId: String = ""

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)

        // ê¶Œí•œ ìš”ì²­ ëŸ°ì²˜ ë“±ë¡
        requestPermissionLauncher = registerForActivityResult(
            ActivityResultContracts.RequestPermission()
        ) { isGranted: Boolean ->
            // ê¶Œí•œ ìš”ì²­ ê²°ê³¼ ì²˜ë¦¬
            if (isGranted) {
                // (!!) ê¶Œí•œ íšë“ ì‹œ ë°”ë¡œ ì‹œì‘
                checkMicrophonePermissionAndStartSTT()
            } else {
                Toast.makeText(requireContext(), "ë§ˆì´í¬ ê¶Œí•œì´ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤.", Toast.LENGTH_SHORT).show()
            }
        }
    }

    override fun onCreateView(
        inflater: LayoutInflater,
        container: ViewGroup?,
        savedInstanceState: Bundle?
    ): View {
        _binding = FragmentAnalysisBinding.inflate(inflater, container, false)
        return binding.root
    }

    override fun onViewCreated(view: View, savedInstanceState: Bundle?) {
        super.onViewCreated(view, savedInstanceState)

        binding.textViewNowspeaking.text = speakingSentence

        // (!!) STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ)
        Thread {
            setupStreamingSTT()
        }.start()

        // ì›¹ì†Œì¼“ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ì—°ê²°
        stompClient = PresentationStompClient(PRESENTATION_ID, ::onHintReceived, ::onProgressReceived)
        stompClient.connect()

        // ì²˜ìŒì—” ì¤‘ë‹¨ ë²„íŠ¼ ìˆ¨ê¸°ê¸°
        binding.imageViewStop.visibility = View.GONE

        // ë§ˆì´í¬ í´ë¦­ ì²˜ë¦¬: STT ì‹œì‘
        binding.imageViewMic.setOnClickListener {
            if (!isListening) {
                // ê¶Œí•œ í™•ì¸ í›„ STT ì‹œì‘
                checkMicrophonePermissionAndStartSTT()
            }
        }

        // ì¤‘ë‹¨ ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬: STT ì¤‘ë‹¨
        binding.imageViewStop.setOnClickListener {
            stopStreamingAudio()
            stompClient.disconnect() // ì›¹ì†Œì¼“ ì—°ê²° í•´ì œ
        }
    }

    /**
     * Google Cloud STT í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
     * (ì¸ì¦ íŒŒì¼ I/Oê°€ ìˆìœ¼ë¯€ë¡œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í˜¸ì¶œí•´ì•¼ í•¨)
     */
    private fun setupStreamingSTT() {
        try {
            // ì¸ì¦ íŒŒì¼(credential.json) ë¡œë”©
            val credentialsStream = requireContext().resources.openRawResource(R.raw.credential)
            val credentials = GoogleCredentials.fromStream(credentialsStream)

            // ì¸ì¦ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ SpeechSettings ìƒì„±
            val speechSettings = SpeechSettings.newBuilder()
                .setCredentialsProvider(FixedCredentialsProvider.create(credentials))
                .build()

            // SpeechClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            speechClient = SpeechClient.create(speechSettings)
            Log.d(TAG, "SpeechClient ì´ˆê¸°í™” ì„±ê³µ")

        } catch (e: Exception) {
            Log.e(TAG, "SpeechClient ì´ˆê¸°í™” ì‹¤íŒ¨", e)
            // ì˜¤ë¥˜
            activity?.runOnUiThread {
                Toast.makeText(requireContext(), "STT ì´ˆê¸°í™” ì‹¤íŒ¨. ì•±ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.", Toast.LENGTH_LONG).show()
            }
        }
    }

    // ë§ˆì´í¬ ê¶Œí•œ í™•ì¸ ë° STT ì‹œì‘ ë¡œì§
    private fun checkMicrophonePermissionAndStartSTT() {
        if (ContextCompat.checkSelfPermission(requireContext(), android.Manifest.permission.RECORD_AUDIO)
            == PackageManager.PERMISSION_GRANTED) {

            // (!!) SpeechClientê°€ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if (speechClient == null) {
                Toast.makeText(requireContext(), "STT ì—”ì§„ì„ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", Toast.LENGTH_SHORT).show()
                Thread { setupStreamingSTT() }.start() // (ì¬ì‹œë„)
                return
            }

            // (!!) ìƒˆ ì‹œì‘ í•¨ìˆ˜ í˜¸ì¶œ
            startStreamingAudio()
        } else {
            requestPermissionLauncher.launch(android.Manifest.permission.RECORD_AUDIO)
        }
    }

    // ê¸°ì¡´ inner class STTListener ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.

    /**
     * Google Cloud STT ì„œë²„ë¡œë¶€í„° ì‹¤ì‹œê°„ ì‘ë‹µ(í…ìŠ¤íŠ¸)ì„ ìˆ˜ì‹ í•˜ëŠ” ì½œë°± ê°ì²´ì…ë‹ˆë‹¤.
     */
    private val responseObserver = object : ApiStreamObserver<StreamingRecognizeResponse> {

        /**
         * ì„œë²„ì—ì„œ STT ê²°ê³¼(ì¤‘ê°„ ë˜ëŠ” ìµœì¢…)ê°€ ë„ì°©í–ˆì„ ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤.
         * (ì´ í•¨ìˆ˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤)
         */
        override fun onNext(response: StreamingRecognizeResponse) {
            // 1. ìœ íš¨í•œ ê²°ê³¼ê°€ ìˆëŠ”ì§€ í™•ì¸
            val result = response.resultsList.firstOrNull()
            if (result == null || result.alternativesList.isEmpty()) {
                return
            }

            // 2. ì¸ì‹ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            val transcript = result.alternativesList[0].transcript.trim()

            // 3. (ì¤‘ìš”) UI ìŠ¤ë ˆë“œë¡œ ì „í™˜í•˜ì—¬ ê¸°ì¡´ ë¡œì§ ì‹¤í–‰
            activity?.runOnUiThread {
                if (result.isFinal) {
                    // --- 'ìµœì¢…' ê²°ê³¼ (onResultsì™€ ìœ ì‚¬) ---
                    Log.d(TAG, "[ìµœì¢…] $transcript")

                    // ê¸°ì¡´ onResults ë¡œì§ (ë²„í¼ ëˆ„ì  ë° ì§„í–‰ë¥  ê³„ì‚°)
                    recognizedSpeechBuffer.append(transcript).append(" ")
                    trimSpeechBufferIfNeeded()
                    val textToSend = recognizedSpeechBuffer.toString().trim()

                    stompClient.sendSttTextForProgress(speakingId, speakingSentence, textToSend)

                } else {
                    // --- 'ì¤‘ê°„' ê²°ê³¼ (onPartialResultsì™€ ìœ ì‚¬) ---
                    Log.d(TAG, "[ì¤‘ê°„] $transcript")

                    // ê¸°ì¡´ onPartialResults ë¡œì§ (ì¡ìŒ í•„í„°ë§ ë° íŒíŠ¸ ìš”ì²­)
                    if (isMeaningfulSpeech(transcript)) {
                        stompClient.sendSttText(transcript) // íŒíŠ¸ ì¶”ì  ìš”ì²­
                        stompClient.sendSttTextForProgress(speakingId, speakingSentence, transcript) // ì§„í–‰ë¥  ì¦‰ì‹œ ë°˜ì˜
                    }
                }
            }
        }

        /** ì˜¤ë¥˜ ë°œìƒ ì‹œ (ê¸°ì¡´ onErrorì™€ ìœ ì‚¬) */
        override fun onError(t: Throwable) {
            Log.e(TAG, "STT ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜", t)
            // (í•„ìš”ì‹œ) ìŠ¤íŠ¸ë¦¬ë° ì¬ì‹œì‘ ë¡œì§
        }

        /** ìŠ¤íŠ¸ë¦¼ì´ ì •ìƒ ì¢…ë£Œë˜ì—ˆì„ ë•Œ */
        override fun onCompleted() {
            Log.d(TAG, "STT ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
        }
    }

    /**
     * STT ê²°ê³¼ê°€ ì¡ìŒì´ë‚˜ ì§§ì€ ê°íƒ„ì‚¬ê°€ ì•„ë‹Œ ìœ ì˜ë¯¸í•œ ë°œí™”ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
     * ì´ í•¨ìˆ˜ê°€ falseë¥¼ ë°˜í™˜í•˜ë©´ ì¹¨ë¬µ íƒ€ì´ë¨¸ê°€ ë¦¬ì…‹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
     * @param text STT ì—”ì§„ìœ¼ë¡œë¶€í„° ìˆ˜ì‹ ëœ í…ìŠ¤íŠ¸
     * @return ìœ ì˜ë¯¸í•˜ë©´ true, ì¡ìŒì„± í…ìŠ¤íŠ¸ë©´ false
     */
    private fun isMeaningfulSpeech(text: String): Boolean {
        // 1. ì „ì²˜ë¦¬: êµ¬ë‘ì ê³¼ ê³µë°±ì„ ì œê±°í•˜ì—¬ ì‹¤ì œ ë‚´ìš©ë¬¼ë§Œ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ ì •ê·œí™”
        // êµ¬ë‘ì ê³¼ ê³µë°±ì„ ì œê±°í•´ë„ í…ìŠ¤íŠ¸ê°€ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
        val normalizedText = text.replace(Regex("[\\s.,?!:;\"'\\-_]"), "").trim()

//        // 2. ìµœì†Œ ê¸¸ì´ ê²€ì‚¬ (ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ê¸°ì¤€)
//        // 2ê¸€ì ë¯¸ë§Œì€ ëŒ€ë¶€ë¶„ ì¡ìŒ (ì˜ˆ: "ì•„", "ìŒ")
//        if (normalizedText.length < 2) {
//            Log.v(TAG, "FILTERED: ì§§ì€ ê¸¸ì´ ($normalizedText)")
//            return false
//        }

        // 3. ë°˜ë³µë˜ëŠ” ë¬¸ìì—´ ê²€ì‚¬ (ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ ê¸°ì¤€)
        // "ã…‹ã…‹ã…‹", "ì•„ì•„ì•„", "......" ë“± ì˜ë¯¸ ì—†ëŠ” ë°˜ë³µ
        if (normalizedText.all { it == normalizedText.first() } && normalizedText.length > 1) {
            Log.v(TAG, "FILTERED: ë°˜ë³µ ë¬¸ìì—´ ($normalizedText)")
            return false
        }

        // 4. ì¡ìŒ/ê°íƒ„ì‚¬ íŒ¨í„´ ê²€ì‚¬
        // 'ì•„', 'ì—', 'ì´', 'ì˜¤', 'ìš°', 'ìŒ', 'í ', 'í' ë“±ìœ¼ë¡œë§Œ ì´ë£¨ì–´ì§„ íŒ¨í„´ (í•œ ê¸€ì ì´ˆê³¼)
        val noisePattern = Regex("^[ì•„ì—ì´ì˜¤ìš°ìŒí í]+$")
        if (normalizedText.matches(noisePattern)) {
            Log.v(TAG, "FILTERED: ê°íƒ„ì‚¬ íŒ¨í„´ ($normalizedText)")
            return false
        }

        // 5. ì¼ë°˜ì ì¸ ì¡ìŒ í‚¤ì›Œë“œ í¬í•¨ ê²€ì‚¬
        val commonNoiseKeywords = listOf("ì½œë¡", "ì—í—´", "ìŒ", "íìŒ", "ì–´", "ì•„", "ìŒ...", "ìŒ...")
        for (keyword in commonNoiseKeywords) {
            if (normalizedText.contains(keyword)) {
                // "ìŒ"ì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ë¼ë„ ê¸¸ì´ê°€ ê¸¸ë©´ ìœ ì˜ë¯¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
                // ê¸¸ì´ê°€ ì§§ê±°ë‚˜ (ì˜ˆ: 4ê¸€ì ë¯¸ë§Œ) í•´ë‹¹ í‚¤ì›Œë“œì™€ ë§¤ìš° ìœ ì‚¬í•  ê²½ìš°ì—ë§Œ í•„í„°ë§
                if (normalizedText.length < 4 || normalizedText == keyword.replace("...", "")) {
                    Log.v(TAG, "FILTERED: ì¼ë°˜ ì¡ìŒ í‚¤ì›Œë“œ í¬í•¨ ($normalizedText)")
                    return false
                }
            }
        }

        // ìœ„ í•„í„°ë¥¼ ëª¨ë‘ í†µê³¼í•˜ë©´ ìœ ì˜ë¯¸í•œ ë°œí™”ë¡œ ê°„ì£¼í•˜ì—¬ íƒ€ì´ë¨¸ ë¦¬ì…‹
        return true
    }

    /**
     * AudioRecordë¥¼ ì‹œì‘í•˜ê³ , STT ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ì„ ì‹œì‘í•©ë‹ˆë‹¤.
     */
    @RequiresPermission(Manifest.permission.RECORD_AUDIO)
    private fun startStreamingAudio() {
        if (isListening) return

        // (ê¶Œí•œ í™•ì¸ ë¡œì§ì€ checkMicrophonePermissionAndStartSTT ì¬í™œìš©)

        // (1) AudioRecord ì´ˆê¸°í™”
        bufferSize = AudioRecord.getMinBufferSize(sampleRate, channelConfig, audioFormat)
        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            sampleRate,
            channelConfig,
            audioFormat,
            bufferSize
        )

        // (2) STT ìŠ¤íŠ¸ë¦¼ ìš”ì²­ ì‹œì‘
        // (responseObserverê°€ ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤)
        requestObserver = speechClient?.streamingRecognizeCallable()?.bidiStreamingCall(responseObserver)

        // (3) STT ìŠ¤íŠ¸ë¦¼ ì„¤ì • ì „ì†¡ (ì–´ë–¤ ì˜¤ë””ì˜¤ì¸ì§€ ì•Œë ¤ì£¼ê¸°)
        val recognitionConfig = RecognitionConfig.newBuilder()
            .setEncoding(RecognitionConfig.AudioEncoding.LINEAR16)
            .setSampleRateHertz(sampleRate)
            .setLanguageCode("ko-KR") // í•œêµ­ì–´ ì„¤ì •
            .setEnableAutomaticPunctuation(true) // ìë™ êµ¬ë‘ì 
            .build()

        val streamingConfig = StreamingRecognitionConfig.newBuilder()
            .setConfig(recognitionConfig)
            .setInterimResults(true) // (í•µì‹¬!) ì¤‘ê°„ ê²°ê³¼ ë°›ê¸°
            .build()

        val initialRequest = StreamingRecognizeRequest.newBuilder()
            .setStreamingConfig(streamingConfig)
            .build()

        requestObserver?.onNext(initialRequest)

        // (4) AudioRecord ë…¹ìŒ ì‹œì‘
        audioRecord?.startRecording()
        isListening = true

        // (5) (í•µì‹¬!) ì˜¤ë””ì˜¤ ì½ê¸°/ì „ì†¡ì„ ìœ„í•œ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        Thread {
            val buffer = ByteArray(bufferSize)
            while (isListening) {
                // ì˜¤ë””ì˜¤ ë²„í¼ ì½ê¸°
                val readSize = audioRecord?.read(buffer, 0, buffer.size) ?: 0

                if (readSize > 0) {
                    // ì½ì€ ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ByteStringìœ¼ë¡œ ë³€í™˜
                    val audioData = ByteString.copyFrom(buffer, 0, readSize)

                    // STT ì„œë²„ë¡œ ì˜¤ë””ì˜¤ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°
                    val request = StreamingRecognizeRequest.newBuilder()
                        .setAudioContent(audioData)
                        .build()
                    requestObserver?.onNext(request)
                }
            }
        }.start()

        // (6) UI ë° íƒ€ì´ë¨¸ ì‹œì‘ (ê¸°ì¡´ ë¡œì§)
        activity?.runOnUiThread {
            binding.imageViewStop.visibility = View.VISIBLE
            Toast.makeText(requireContext(), "ë°œí‘œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.", Toast.LENGTH_SHORT).show()
        }
    }

    /**
     * AudioRecordë¥¼ ì¤‘ì§€í•˜ê³ , STT ìŠ¤íŠ¸ë¦¬ë°ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
     */
    private fun stopStreamingAudio() {
        if (!isListening) return

        isListening = false

        // (1) AudioRecord ì¤‘ì§€ ë° í•´ì œ
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null

        // (2) STT ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì•Œë¦¼
        requestObserver?.onCompleted()
        requestObserver = null

        // (3) UI ë° íƒ€ì´ë¨¸ ì¤‘ì§€ (ê¸°ì¡´ ë¡œì§)
        binding.imageViewStop.visibility = View.GONE
        Toast.makeText(requireContext(), "ë°œí‘œê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.", Toast.LENGTH_SHORT).show()
        cancelHintTimer()

    }

    private fun cancelHintTimer() {
        hintHandler.removeCallbacks(hintTimerRunnable)
    }

    // ì›¹ì†Œì¼“ìœ¼ë¡œ íŒíŠ¸ ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í–ˆì„ ë•Œ ì‹¤í–‰ë  ì½œë°± í•¨ìˆ˜
    private fun onHintReceived(response: SimilarityResponse) {
        Log.d(TAG, "ì„œë²„ì—ì„œ íŒíŠ¸ ìˆ˜ì‹ : ${response.mostSimilarId}")
        if (isAdded) {
            // íŒíŠ¸ë¥¼ UIì— í‘œì‹œ
            binding.textViewResult.text = ""
            binding.textViewResult.text = response.mostSimilarText

            speakingSentence = response.mostSimilarText
            speakingId = response.mostSimilarId
            binding.textViewNowspeaking.text = "í˜„ì¬ ë°œí™” ì¤‘ì¸ ë¬¸ì¥: \n ${speakingSentence}"

        }
    }

    // ì§„í–‰ë¥  ê³„ì‚° ê²°ê³¼ ìˆ˜ì‹ í–ˆì„ ë•Œ
    private fun onProgressReceived(progress: ProgressResponse){
        Log.d(TAG, "ì„œë²„ì—ì„œ ì§„í–‰ë¥  ê³„ì‚° ê²°ê³¼ ìˆ˜ì‹ : ${progress.nextScriptId}")
        if (isAdded) {
            // ì§„í–‰ë¥  UIì— í‘œì‹œ(ì„ì‹œ)
            binding.textViewProgress.text = ""
            binding.textViewProgress.text = ("ë‹¤ìŒ ë¬¸ì¥ id: ${progress.nextScriptId}")
        }
    }

    override fun onDestroyView() {
        super.onDestroyView()

        // ì›¹ì†Œì¼“ ì—°ê²° í•´ì œ
        if (::stompClient.isInitialized) {
            stompClient.disconnect()
        }

        // (!!) ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€
        if (isListening) {
            stopStreamingAudio()
        }

        // (!!) STT í´ë¼ì´ì–¸íŠ¸ í•´ì œ
        speechClient?.shutdown()
        speechClient?.awaitTermination(5, java.util.concurrent.TimeUnit.SECONDS)
        speechClient = null

        cancelHintTimer()
        _binding = null
    }

    private fun trimSpeechBufferIfNeeded() {
        // ë²„í¼ë¥¼ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
        val words = recognizedSpeechBuffer.toString().trim().split("\\s+".toRegex())

        if (words.size > MAX_WORD_COUNT) {
            Log.d(TAG, "ë²„í¼ ë‹¨ì–´ ìˆ˜ ì´ˆê³¼ (${words.size}ê°œ). ì•ë¶€ë¶„ ${TRIM_WORD_COUNT}ê°œ ì‚­ì œ.")

            // ìµœì‹  ë‚´ìš© (words.size - TRIM_WORD_COUNT)ê°œë§Œ ìœ ì§€
            val newWords = words.subList(TRIM_WORD_COUNT, words.size)

            // ë²„í¼ë¥¼ ìƒˆë¡œìš´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ì¬êµ¬ì„±
            recognizedSpeechBuffer.clear()
            recognizedSpeechBuffer.append(newWords.joinToString(" "))
        }
    }
}